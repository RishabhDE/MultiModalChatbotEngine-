{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with GPT-4.0-mini\n",
    "By training with GPT-4.0-mini, we aim to achieve better performance and more natural conversations in our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up OpenAI API Client and Generating a Chat Completion\n",
    "\n",
    "In this section, we will set up the OpenAI API client and generate a chat completion using the GPT-4 model. The steps include:\n",
    "\n",
    "1. **Importing Required Libraries**: We import necessary libraries such as `Markdown` and `display` from `IPython.display`, `OpenAI` from `openai`, and `os` and `getpass` for handling environment variables and secure input.\n",
    "\n",
    "2. **Entering API Key**: We securely input the OpenAI API key using `getpass.getpass()` to avoid exposing the key in the code.\n",
    "\n",
    "3. **Initializing OpenAI Client**: We initialize the OpenAI client with the provided API key.\n",
    "\n",
    "4. **Creating a Chat Completion**: We create a chat completion using the `client.chat.completions.create()` method. The model used is `gpt-4o-mini`, and the messages include a system message setting the context and a user message asking about the future of AI.\n",
    "\n",
    "5. **Displaying the Response**: Finally, we display the response using `Markdown` to render it in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key entered successfully!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The future of AI is expected to be shaped by several key trends and developments across various sectors:\n",
       "\n",
       "1. **Enhanced Integration**: AI will become more integrated into our daily lives, enhancing everything from personal assistants to complex business systems. This integration will lead to more seamless user experiences and improved efficiencies.\n",
       "\n",
       "2. **Ethical AI**: As AI becomes more powerful, the importance of ethics in AI development will grow. Organizations will focus on creating fair, transparent, and accountable AI systems to avoid biases and ensure that the technology benefits all segments of society.\n",
       "\n",
       "3. **AI and Sustainability**: AI is poised to play a significant role in addressing climate change and sustainability challenges. It can optimize energy consumption, enhance resource management, and facilitate the development of green technologies.\n",
       "\n",
       "4. **Advancements in Natural Language Processing (NLP)**: We can expect further breakthroughs in NLP, making AI systems more capable of understanding and generating human-like text, which can enhance communication and information retrieval.\n",
       "\n",
       "5. **AI in Healthcare**: AI's application in healthcare will expand, from diagnostics to personalized medicine. Predictive analytics can lead to better patient outcomes and more efficient healthcare delivery.\n",
       "\n",
       "6. **Autonomous Systems**: The development of autonomous systems, including self-driving vehicles and drones, will continue. This raises important regulatory, safety, and ethical considerations that will shape how these technologies are implemented.\n",
       "\n",
       "7. **Human-AI Collaboration**: Instead of replacing jobs, AI will increasingly serve as a tool for augmenting human capabilities. This partnership will enhance productivity and creativity across various professions.\n",
       "\n",
       "8. **Democratization of AI**: Tools and platforms for developing AI will become more accessible to a wider audience, allowing individuals and small businesses to leverage AI without needing advanced technical skills.\n",
       "\n",
       "9. **Focus on General AI**: While current AI systems are designed for specific tasks (narrow AI), research into artificial general intelligence (AGI) will continue, aiming to create AI that can understand, learn, and apply knowledge across a wide range of tasks similar to human intelligence.\n",
       "\n",
       "10. **Regulatory Frameworks**: As AI technologies evolve, governments and international bodies will likely establish new regulations to govern their development and use. This could help manage risks associated with AI, including privacy concerns and job displacement.\n",
       "\n",
       "Overall, the future of AI holds immense potential for innovation and improvement in many areas, but it also requires careful consideration of ethical implications and societal impacts. Balancing progress with responsibility will be crucial in shaping a positive future for AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass(\"Enter your API key: \")\n",
    "print(\"API key entered successfully!\")\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a professor at University of San Diego.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the future of AI?\"}\n",
    "  ]\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning an OpenAI Model and the Use of JSONL Files\n",
    "\n",
    "In this section, we will walk through the process of fine-tuning an OpenAI model using a dataset of movie conversations. We will also discuss the importance of converting data to JSONL format and what JSONL files are.\n",
    "\n",
    "#### Fine-Tuning an OpenAI Model\n",
    "\n",
    "Fine-tuning is the process of taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task. This allows the model to learn the nuances and patterns specific to the new dataset, improving its performance on related tasks.\n",
    "\n",
    "The steps involved in fine-tuning an OpenAI model are as follows:\n",
    "\n",
    "1. **Prepare the Dataset**: The dataset needs to be in a format that the model can understand. For OpenAI models, this often means converting the data into JSONL (JSON Lines) format.\n",
    "2. **Upload the Dataset**: The prepared dataset is uploaded to OpenAI's servers.\n",
    "3. **Create a Fine-Tuning Job**: A fine-tuning job is created using the uploaded dataset. This job specifies the model to be fine-tuned, the dataset to use, and any hyperparameters for the training process.\n",
    "4. **Monitor the Fine-Tuning Process**: The status of the fine-tuning job can be monitored to ensure it is progressing as expected.\n",
    "5. **Retrieve the Fine-Tuned Model**: Once the fine-tuning process is complete, the fine-tuned model can be retrieved and used for inference.\n",
    "\n",
    "#### Why Convert Data to JSONL Files?\n",
    "\n",
    "JSONL (JSON Lines) is a convenient format for storing structured data where each line is a valid JSON object. This format is particularly useful for machine learning tasks for several reasons:\n",
    "\n",
    "- **Line-by-Line Processing**: Each line in a JSONL file represents a separate JSON object, making it easy to process large datasets line by line without loading the entire file into memory.\n",
    "- **Flexibility**: JSONL files can store complex nested structures, making them suitable for a wide range of data types.\n",
    "- **Compatibility**: Many machine learning frameworks and tools, including OpenAI's API, support JSONL format, making it a standard choice for data interchange.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset saved to movie_conversation_train.jsonl\n",
      "Validation dataset saved to movie_conversation_validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define file paths\n",
    "movie_lines_file = \"movie_lines.txt\"\n",
    "movie_conversations_file = \"movie_conversations.txt\"\n",
    "\n",
    "# Step 1: Load the lines from movie_lines.txt\n",
    "lines_dict = {}\n",
    "with open(movie_lines_file, 'r', encoding='iso-8859-1') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 5:  # Ensuring the correct format\n",
    "            line_id, character_id, movie_id, character_name, text = parts\n",
    "            lines_dict[line_id] = text\n",
    "\n",
    "# Step 2: Load conversations from movie_conversations.txt\n",
    "conversations = []\n",
    "with open(movie_conversations_file, 'r', encoding='iso-8859-1') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 4:  # Ensuring the correct format\n",
    "            line_ids_str = parts[3]\n",
    "            line_ids = re.findall(r\"L[0-9]+\", line_ids_str)  # Extracting line IDs\n",
    "            conversation = [lines_dict[line_id] for line_id in line_ids if line_id in lines_dict]\n",
    "            if len(conversation) > 1:  # Only add conversations with multiple lines\n",
    "                conversations.append(conversation)\n",
    "\n",
    "# Prepare the data for training\n",
    "jsonl_data = []\n",
    "for conversation in conversations:\n",
    "    jsonl_data.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Engage in a conversation based on movie dialogues.\"},\n",
    "            {\"role\": \"user\", \"content\": conversation[0]},  # First line as user input\n",
    "            {\"role\": \"assistant\", \"content\": conversation[1]}  # Second line as assistant response\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(jsonl_data)\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "train_data, validation_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def save_to_jsonl(data, output_file_path):\n",
    "    # Save to JSONL format\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for _, row in data.iterrows():\n",
    "            f.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "# Save the training and validation sets to separate JSONL files\n",
    "train_output_file_path = 'movie_conversation_train.jsonl' \n",
    "validation_output_file_path = 'movie_conversation_validation.jsonl'\n",
    "\n",
    "save_to_jsonl(train_data, train_output_file_path)\n",
    "save_to_jsonl(validation_data, validation_output_file_path)\n",
    "\n",
    "print(f\"Training dataset saved to {train_output_file_path}\")\n",
    "print(f\"Validation dataset saved to {validation_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Training and Validation Files for Fine-Tuning\n",
    "\n",
    "In this section, we will upload the training and validation datasets to OpenAI's servers to prepare for fine-tuning the model. The datasets are in JSONL format, which is required for the fine-tuning process.\n",
    "\n",
    "The steps involved are as follows:\n",
    "\n",
    "1. **Upload Training File**: We use the `client.files.create()` method to upload the training dataset. The file is opened in binary read mode (`\"rb\"`) and the purpose is set to `\"fine-tune\"`.\n",
    "\n",
    "2. **Upload Validation File**: Similarly, we upload the validation dataset using the same method.\n",
    "\n",
    "3. **Print File Information**: After uploading, we print the information of both the training and validation files to confirm successful uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file Info: FileObject(id='file-Hs5HujD5LxnH63f5ewQWK0o7', bytes=18403816, created_at=1729157290, filename='movie_conversation_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Validation file Info: FileObject(id='file-k5g9mrkMXhZvuBwYnALcDFaX', bytes=4602260, created_at=1729157293, filename='movie_conversation_validation.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "train_file = client.files.create(\n",
    "  file=open(train_output_file_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "valid_file = client.files.create(\n",
    "  file=open(validation_output_file_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(f\"Training file Info: {train_file}\")\n",
    "print(f\"Validation file Info: {valid_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Fine-Tuning Job\n",
    "\n",
    "In this section, we will create a fine-tuning job using the OpenAI API. The fine-tuning job will use the training and validation datasets we previously uploaded. We will specify the model to be fine-tuned and set the hyperparameters for the training process.\n",
    "\n",
    "The steps involved are as follows:\n",
    "\n",
    "1. **Specify Training and Validation Files**: We use the IDs of the training and validation files that were uploaded to OpenAI's servers.\n",
    "2. **Set Model and Hyperparameters**: We specify the model to be fine-tuned (`gpt-4o-mini-2024-07-18`) and set the hyperparameters, including the number of epochs, batch size, and learning rate multiplier.\n",
    "3. **Create Fine-Tuning Job**: We create the fine-tuning job using the `client.fine_tuning.jobs.create()` method.\n",
    "4. **Retrieve Job ID and Status**: After creating the job, we retrieve the job ID and status to monitor the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with jobID: ftjob-nJMvXlFf73BUJKCrX7jkpuLO.\n",
      "Training Response: FineTuningJob(id='ftjob-nJMvXlFf73BUJKCrX7jkpuLO', created_at=1729157330, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=3, learning_rate_multiplier=0.3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-pZiumcjzaUnCXxgMZCyV8P9C', result_files=[], seed=537079073, status='validating_files', trained_tokens=None, training_file='file-Hs5HujD5LxnH63f5ewQWK0o7', validation_file='file-k5g9mrkMXhZvuBwYnALcDFaX', estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Training Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "model = client.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id, \n",
    "  validation_file=valid_file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\", \n",
    "  hyperparameters={\n",
    "    \"n_epochs\": 3,\n",
    "\t\"batch_size\": 3,\n",
    "\t\"learning_rate_multiplier\": 0.3\n",
    "  }\n",
    ")\n",
    "job_id = model.id\n",
    "status = model.status\n",
    "\n",
    "print(f'Fine-tuning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {model}\")\n",
    "print(f\"Training Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-nJMvXlFf73BUJKCrX7jkpuLO\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Fine-Tuned Model: ft:gpt-4o-mini-2024-07-18:personal::AJWkdy4H\n",
      "Status: succeeded\n",
      "Created At: 2024-10-17 09:28:50\n",
      "Finished At: 2024-10-18 02:11:29\n",
      "Training File: file-Hs5HujD5LxnH63f5ewQWK0o7\n",
      "Validation File: file-k5g9mrkMXhZvuBwYnALcDFaX\n",
      "Hyperparameters: Hyperparameters(n_epochs=3, batch_size=3, learning_rate_multiplier=0.3)\n",
      "Trained Tokens: 9891729\n",
      "Result Files: ['file-4AFc3bMpt4K0KE90P6g46Xvu']\n",
      "Error: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "job = client.fine_tuning.jobs.retrieve(\"ftjob-nJMvXlFf73BUJKCrX7jkpuLO\")\n",
    "\n",
    "# Display the job details in a readable format\n",
    "print(f\"Job ID: {job.id}\")\n",
    "print(f\"Model: {job.model}\")\n",
    "print(f\"Fine-Tuned Model: {job.fine_tuned_model}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "print(f\"Created At: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(job.created_at))}\")\n",
    "print(f\"Finished At: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(job.finished_at))}\")\n",
    "print(f\"Training File: {job.training_file}\")\n",
    "print(f\"Validation File: {job.validation_file}\")\n",
    "print(f\"Hyperparameters: {job.hyperparameters}\")\n",
    "print(f\"Trained Tokens: {job.trained_tokens}\")\n",
    "print(f\"Result Files: {job.result_files}\")\n",
    "print(f\"Error: {job.error.message if job.error.code else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: succeeded\n",
      "Hyperparameters: Hyperparameters(n_epochs=3, batch_size=3, learning_rate_multiplier=0.3)\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Training File: file-Hs5HujD5LxnH63f5ewQWK0o7\n",
      "Validation File: file-k5g9mrkMXhZvuBwYnALcDFaX\n",
      "Fine-tuning job has completed.\n",
      "Status: succeeded\n",
      "Hyperparameters: Hyperparameters(n_epochs=3, batch_size=3, learning_rate_multiplier=0.3)\n",
      "Model: gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to retrieve and print fine-tuning job details including loss\n",
    "def check_fine_tuning_job_status(client, job_id):\n",
    "    while True:\n",
    "        # Retrieve the state of the fine-tuning job\n",
    "        job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "        # Print relevant details\n",
    "        print(f\"Status: {job.status}\")\n",
    "        print(f\"Hyperparameters: {job.hyperparameters}\")\n",
    "        print(f\"Model: {job.model}\")\n",
    "        print(f\"Training File: {job.training_file}\")\n",
    "        print(f\"Validation File: {job.validation_file}\")\n",
    "\n",
    "        # Check for error status\n",
    "        if job.error.code is not None:\n",
    "            print(f\"Error: {job.error.message}\")\n",
    "            break\n",
    "\n",
    "        # Check if the job has finished\n",
    "        if job.status in ['succeeded', 'failed', 'cancelled']:\n",
    "            print(\"Fine-tuning job has completed.\")\n",
    "            break\n",
    "        \n",
    "        # Optionally retrieve training metrics like loss\n",
    "        if job.status in ['training', 'validating_files']:  # Check while in training\n",
    "            if hasattr(job, 'loss'):\n",
    "                print(f\"Current Loss: {job.loss}\")  # Adjust based on actual API response structure\n",
    "\n",
    "        # Wait for a while before checking again\n",
    "        print(\"Waiting for the job to finish...\")\n",
    "        time.sleep(10)  # Check every 10 seconds\n",
    "\n",
    "job_id = 'ftjob-nJMvXlFf73BUJKCrX7jkpuLO' \n",
    "check_fine_tuning_job_status(client, job_id)\n",
    "\n",
    "print(f\"Status: {job.status}\")\n",
    "print(f\"Hyperparameters: {job.hyperparameters}\")\n",
    "print(f\"Model: {job.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'data': [{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_lZX1fMnF4x0Wfo0Ok2d53GFK', 'created_at': 1729217486, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AJWkdy4H', 'fine_tuning_job_id': 'ftjob-nJMvXlFf73BUJKCrX7jkpuLO', 'metrics': {'step': 66427, 'train_loss': 1.9935097694396973, 'train_mean_token_accuracy': 0.5277777910232544}, 'step_number': 66427}, {'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_nmrDhwfYLautzFgIXyrfhXDK', 'created_at': 1729217398, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AJWkdxWX:ckpt-step-66426', 'fine_tuning_job_id': 'ftjob-nJMvXlFf73BUJKCrX7jkpuLO', 'metrics': {'step': 66426, 'train_loss': 2.293053388595581, 'train_mean_token_accuracy': 0.49152541160583496}, 'step_number': 66426}, {'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_VzR8v7a5e5YiNXqiCQXQjZ7J', 'created_at': 1729198028, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AJWkdVEz:ckpt-step-44284', 'fine_tuning_job_id': 'ftjob-nJMvXlFf73BUJKCrX7jkpuLO', 'metrics': {'step': 44284, 'train_loss': 2.1637237071990967, 'train_mean_token_accuracy': 0.5416666865348816}, 'step_number': 44284}], 'has_more': False, 'first_id': 'ftckpt_lZX1fMnF4x0Wfo0Ok2d53GFK', 'last_id': 'ftckpt_VzR8v7a5e5YiNXqiCQXQjZ7J'}\n",
      "Step Number: 66427\n",
      "Train Loss: 1.9935097694396973\n",
      "Mean Token Accuracy: 0.5277777910232544\n",
      "CheckPoint Name: ft:gpt-4o-mini-2024-07-18:personal::AJWkdy4H\n",
      "------\n",
      "Step Number: 66426\n",
      "Train Loss: 2.293053388595581\n",
      "Mean Token Accuracy: 0.49152541160583496\n",
      "CheckPoint Name: ft:gpt-4o-mini-2024-07-18:personal::AJWkdxWX:ckpt-step-66426\n",
      "------\n",
      "Step Number: 44284\n",
      "Train Loss: 2.1637237071990967\n",
      "Mean Token Accuracy: 0.5416666865348816\n",
      "CheckPoint Name: ft:gpt-4o-mini-2024-07-18:personal::AJWkdVEz:ckpt-step-44284\n",
      "------\n",
      "No more checkpoints to retrieve.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# Function to retrieve and print checkpoints for the fine-tuning job\n",
    "def check_fine_tuning_job_checkpoints(job_id):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        # Call the OpenAI API to retrieve checkpoints\n",
    "        response = requests.get(f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\", headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(data)\n",
    "            # Process and print each checkpoint\n",
    "            for checkpoint in data['data']:\n",
    "                step_number = checkpoint['step_number']\n",
    "                train_loss = checkpoint['metrics'].get('train_loss')\n",
    "                full_valid_mean_token_accuracy = checkpoint['metrics'].get('train_mean_token_accuracy')\n",
    "                fine_tuned_model_checkpoint = checkpoint.get('fine_tuned_model_checkpoint')\n",
    "                # Print relevant details for each checkpoint\n",
    "                print(f\"Step Number: {step_number}\")\n",
    "                print(f\"Train Loss: {train_loss}\")\n",
    "                print(f\"Mean Token Accuracy: {full_valid_mean_token_accuracy}\")\n",
    "                \n",
    "                print(f\"CheckPoint Name: {fine_tuned_model_checkpoint}\")\n",
    "                print(\"------\")\n",
    "                \n",
    "            # Check if there are more checkpoints to retrieve\n",
    "            if not data.get(\"has_more\", False):\n",
    "                print(\"No more checkpoints to retrieve.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Error retrieving checkpoints: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        # Wait before checking again\n",
    "        print(\"Waiting for the next checkpoint...\")\n",
    "        time.sleep(10)  # Adjust the wait time as needed\n",
    "\n",
    "# Call the function to start checking for checkpoints\n",
    "check_fine_tuning_job_checkpoints(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Fine-Tuned Model: ft:gpt-4o-mini-2024-07-18:personal::AJWkdy4H\n",
      "You: Hey! You went to see her yesterday?\n",
      "Assistant: I sure did. I had to explain things to her. I said, we're friends. You know, you just gotta be sure that everybody you care about can get along forever. You can't have bad feelings about people.\n",
      "You: What she told you?\n",
      "Assistant: Well, how nervous she was. She wanted to tell me how nervous she was.\n",
      "You: And why she was nervous?\n",
      "Assistant: Knowing that she was, you know, on the eve of something. She sure is uptight.\n",
      "You: Let her rest for sometime\n",
      "Assistant: I went to pick her up at the train station, she's being so... Nervous.\n",
      "Ending the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Retrieve the list of fine-tuning jobs using the client and get the fine-tuned model\n",
    "result = client.fine_tuning.jobs.list()\n",
    "fine_tuned_model = \"ft:gpt-4o-mini-2024-07-18:personal::AJWkdy4H\"  # Use the correct key to get fine-tuned model name\n",
    "print(f\"Using Fine-Tuned Model: {fine_tuned_model}\")\n",
    "\n",
    "# Initialize the conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Given a social media post, classify whether it indicates 'stress' or 'non-stress'.\"}\n",
    "]\n",
    "\n",
    "# Chat loop with the fine-tuned model\n",
    "while True:\n",
    "    # User input prompt\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Check for exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Ending the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Append user message to conversation history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Call the OpenAI chat completion API using the client object\n",
    "    response = client.chat.completions.create(\n",
    "        model=fine_tuned_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Retrieve and display the model's response\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Assistant: {assistant_message}\")\n",
    "\n",
    "    # Append the assistant's response to conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
