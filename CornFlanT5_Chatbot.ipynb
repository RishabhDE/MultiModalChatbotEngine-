{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0n9lSAYFoZu"
      },
      "source": [
        "##18. Install & Import Necessary Libraries for Inference\n",
        "* The trained model is now available for running the chatbot. This section of the code sets up the necessary environment and importing the required libraries.\n",
        "* The Gradio library is used to create simple and interactive web interface with the chatbot. The package is installed and imported in this section.\n",
        "* The other libraries are imported to support the trained model execution, with purpose for each library commented in the codebase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR33Z5TfH75x",
        "outputId": "b2bc478e-abb6-42b6-851d-ffacc1f020cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Gradio in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.115.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (1.4.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.26.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.10.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.7.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->Gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->Gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->Gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->Gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->Gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->Gradio) (0.40.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->Gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->Gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->Gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->Gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->Gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->Gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->Gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->Gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->Gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->Gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->Gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->Gradio) (13.9.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->Gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->Gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->Gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->Gradio) (0.1.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Gradio, For User-Friendly Web Interface to Interact with the chatbot\n",
        "!pip install Gradio\n",
        "# SentencePiece for tokenizing inputs to T5 model\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Import necessary libraries\n",
        "import os             # For loading the trained model\n",
        "import re             # For cleaning input text\n",
        "import torch          # Tensor computation and Model handling\n",
        "import gradio as gr   # Gradio for building web interface\n",
        "\n",
        "# Import T5 model and tokenizer from Hugging face library\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLlTI0BibADm"
      },
      "source": [
        "##19. Storage Access for Trained Model\n",
        "* This code was developed predominantly on Google Colab environment. The trained model was stored at Google Drive. This section of the code provides the code Google Dive access with relevant user credentials to load the model trained in above sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6DUAX9-INSd",
        "outputId": "af6bdd72-283a-43af-f3ab-d6932e07ad22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Data\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive for accessing Complaints dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cl6VyPtks2r"
      },
      "source": [
        "##20. Chatbot Response Generator with History Depth Manager\n",
        "* This section of the code provides the response generator of the multi-turn conversational chatbot using the conversation history managed with controlled depth of exhanges and tokens as defined by the text or gui interface.\n",
        "* The generate_response_with_history function generates response from concatenated avaialble history text input. The response is decoded into human readable text without any special tokens. The nature of response is controlled by the following parameters:\n",
        "  * temperature: Controls the randomness of the model's response. Lower values such as 0.7 makes the response factual, while higher values increase randomness in output.\n",
        "  * num_beams: Number of explored beams for potential response. Higher the beams improves the response diversity.\n",
        "  * top_k: Number of top likely tokens to be sampled.\n",
        "  * top_p: Cumulative probability for choosing tokens based on Nucleus sampling.\n",
        "  * rep_penality: Penality for repeating token.\n",
        "* The truncate_conversation_history function ensures that the conversation stays within specified token lenght limit. If the conversation history contains more tokens oldest conversation is popped out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkWh5YnjIQZ-"
      },
      "outputs": [],
      "source": [
        "# Generate response from trained model, considering conversation history\n",
        "def generate_response_with_history(conversation_history, max_length,\n",
        "                                   temperature, num_beams, top_k, top_p,\n",
        "                                   rep_penalty):\n",
        "\n",
        "    # Concatenate the dialogue exchanges without speaker tag\n",
        "    input_text = \" \".join([text.split(\": \")[1] \\\n",
        "                           for text in conversation_history])\n",
        "\n",
        "    # Tokenize the concatenated input text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate a response without gradient calculation\n",
        "    with torch.no_grad():\n",
        "        # Model response guided by the parameters\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            # Beam search only if sampling is off\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            # Controls randomness\n",
        "            temperature=temperature,\n",
        "            # Limits pool to top k tokens\n",
        "            top_k=top_k,\n",
        "            # Nucleus sampling, cumulative probability\n",
        "            top_p=top_p,\n",
        "            # Penalty for repeated phrases\n",
        "            repetition_penalty=rep_penalty,\n",
        "            # Enable sampling only if temperature is altered\n",
        "            do_sample=(temperature != 1.0)\n",
        "        )\n",
        "\n",
        "    # Decode output tokens to return human readable string without special token\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Truncate conversation history if it exceeds thershold\n",
        "def truncate_conversation_history(conversation_history, \\\n",
        "                                  tokenizer, max_token_length):\n",
        "    # Tokenize the conversation history in excluding speaker tags\n",
        "    tokenized_history = tokenizer.encode(\" \".join([text.split(\": \")[1]\n",
        "                    for text in conversation_history]), return_tensors=\"pt\")\n",
        "\n",
        "    # If tokenized history exceeds max_token_length, remove older conversation\n",
        "    while tokenized_history.shape[1] > max_token_length and \\\n",
        "                                          len(conversation_history) > 1:\n",
        "        conversation_history.pop(0)  # Remove the oldest conversation\n",
        "        tokenized_history = tokenizer.encode(\" \".join([text.split(\": \")[1]\n",
        "                    for text in conversation_history]), return_tensors=\"pt\")\n",
        "\n",
        "    return conversation_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TPl7CzRrh_K"
      },
      "source": [
        "##21. Text Interface to Chatbot\n",
        "* This section of the code provides text based interface with the chatbot using the terminal.\n",
        "* The pre-trained model and tokenizer are loaded from the directory. The text interface captures the plain text input, updates the conversation history, limits the conversation history depth as per the defined parameter set, which allows multi turn contextual conversation.\n",
        "* The conversation loop collects the response as defined by the parameter set and prints the response to the user, and adds it to the conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO_3TK3QITD0",
        "outputId": "589da08e-1a80-4373-b076-ffaaaad36177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4\n",
            "Using device: cuda\n",
            "I'm a FLAN-T5 model! I will talk to you till you say 'bye' :)\n",
            "You: What a beautiful day\n",
            "Chatbot: I had a lovely day. The sun was shining and the weather was beautiful.\n",
            "You: What did you do today?\n",
            "Chatbot: I went to the beach.\n",
            "You: Which beach?\n",
            "Chatbot: The beach at the north end of town\n",
            "You: How far it is from here?\n",
            "Chatbot: 3 miles\n",
            "You: Did you eat anything at beach?\n",
            "Chatbot: I didn't eat anything\n",
            "You: What is your favorite dish?\n",
            "Chatbot: Potato salad\n",
            "You: How to prepare potato salad?\n",
            "Chatbot: To prepare potato salad, first cut the potatoes in half lengthwise and place them on a cutting board lined with foil.\n",
            "You: Should we add tomato?\n",
            "Chatbot: I don't think so.\n",
            "You: Shall we go for a drive?\n",
            "Chatbot: I don't think so.\n",
            "You: Bye\n",
            "Ending conversation. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Text Interface to Chatbot\n",
        "\n",
        "# Path to the directory where the trained model is saved\n",
        "model_path = './flan_t5B_cornell_150k'\n",
        "# Load the tokenizer from the directory\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "# Load the trained model from the directory\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Check if a GPU is available and use it; otherwise, fallback to CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the GPU or CPU device\n",
        "model = model.to(device)\n",
        "\n",
        "# Parameter definitions for Response Control\n",
        "params = {\n",
        "    # Maximum number of exchanges to retain in the conversation history\n",
        "    'max_hist_conv': 10,\n",
        "    # Maximum token length for conversation history\n",
        "    'max_hist_token': 512,\n",
        "    # Maximum token length for responses\n",
        "    'max_resp_token': 50,\n",
        "    # Controls response randomness: Higher the value more the randomness\n",
        "    'temperature': 0.95,\n",
        "    # Beam search size : Diversity control\n",
        "    'num_beams': 5,\n",
        "    # Top-k sampling: Number of most likely tokens\n",
        "    'top_k': 50,\n",
        "    # Nucleus sampling: Cumulative Probability\n",
        "    'top_p': 0.85,\n",
        "    # Penalty for repeated phrases: Discourages repetition\n",
        "    'rep_penalty': 5.0\n",
        "}\n",
        "\n",
        "# Chat loop with conversation history\n",
        "print(\"I'm a FLAN-T5 model! I will talk to you till you say 'bye' :)\")\n",
        "conversation_history = []  # Initialize empty conversation history list\n",
        "\n",
        "# Main conversation loop\n",
        "while True:\n",
        "    raw_input = input(\"You: \")\n",
        "    # Sanitize user input by removing special characters\n",
        "    user_input = re.sub(r'[^\\w\\s!?.,]', '', raw_input)\n",
        "\n",
        "    # Check if user wants to end the conversation\n",
        "    if user_input.lower() == 'bye':\n",
        "        print(\"Ending conversation. Goodbye!\")\n",
        "        break  # Exit the loop to end conversation\n",
        "\n",
        "    # Add the user's input to the conversation history with 'You:' tag\n",
        "    conversation_history.append(f\"You: {user_input}\")\n",
        "\n",
        "    # Generate the chatbot response using updated history\n",
        "    response = generate_response_with_history(conversation_history,\n",
        "                        params['max_resp_token'], params['temperature'],\n",
        "                        params['num_beams'], params['top_k'],\n",
        "                        params['top_p'], params['rep_penalty'])\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "    # Add the model's response to the conversation history\n",
        "    conversation_history.append(f\"Me: {response}\")\n",
        "\n",
        "    # Keep only the last max_hist_conv exchanges\n",
        "    if len(conversation_history) > params['max_hist_conv']:\n",
        "        conversation_history = conversation_history[-params['max_hist_conv']:]\n",
        "\n",
        "    # Truncate the conversation history if token length exceeds max_hist_token\n",
        "    conversation_history = truncate_conversation_history(conversation_history,\n",
        "                            tokenizer, params['max_hist_token'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhaXh6KF_pc_"
      },
      "source": [
        "##22. Web GUI Interface to Chatbot\n",
        "* This section of the code provides Web GUI based interface using Gradio to interact with the chatbot.\n",
        "* The pre-trained model and tokenizer are loaded from the directory. The text interface captures the plain text input, updates the conversation history, limits the conversation history depth as per the defined parameter set, which allows multi turn contextual conversation.\n",
        "* The conversation loop collects the response as defined by the parameter set and prints the response to the user, and adds it to the conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "ja2Kag7BIahk",
        "outputId": "0dd57089-d427-4445-8213-4841153d9b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4\n",
            "Using device: cuda\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e022a8814ddad2e9bc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e022a8814ddad2e9bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Web-GUI Interface to Chatbot\n",
        "\n",
        "# Path to the directory where the trained model is saved\n",
        "model_path = './flan_t5B_cornell_150k'\n",
        "# Load the tokenizer from the directory\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "# Load the trained model from the directory\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Check if a GPU is available and use it; otherwise, fallback to CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the GPU or CPU device\n",
        "model = model.to(device)\n",
        "\n",
        "# Parameter definitions for Response Control\n",
        "params = {\n",
        "    # Maximum number of exchanges to retain in the conversation history\n",
        "    'max_hist_conv': 10,\n",
        "    # Maximum token length for conversation history\n",
        "    'max_hist_token': 512,\n",
        "    # Maximum token length for responses\n",
        "    'max_resp_token': 50,\n",
        "    # Controls response randomness: Higher the value more the randomness\n",
        "    'temperature': 0.95,\n",
        "    # Beam search size : Diversity control\n",
        "    'num_beams': 5,\n",
        "    # Top-k sampling: Number of most likely tokens\n",
        "    'top_k': 50,\n",
        "    # Nucleus sampling: Cumulative Probability\n",
        "    'top_p': 0.85,\n",
        "    # Penalty for repeated phrases: Discourages repetition\n",
        "    'rep_penalty': 5.0\n",
        "}\n",
        "\n",
        "# Chat loop with conversation history\n",
        "# print(\"I'm a FLAN-T5 model! I will talk to you till you say 'bye' :)\")\n",
        "conversation_history = []  # Initialize empty conversation history list\n",
        "\n",
        "# Chatbot response function to handle user Input and generate model responses\n",
        "def chatbot(user_input):\n",
        "    global conversation_history # Access global conversation history variable\n",
        "\n",
        "    # Check if user Terminates conversation with 'bye'\n",
        "    if user_input.strip().lower() == \"bye\":\n",
        "        # Close the Gradio interface after responding\n",
        "        iface.close()  # Close the interface\n",
        "        return \"Goodbye! It was nice talking to you.\" #Farewell Message\n",
        "\n",
        "    # Add user input to conversation history with prefix \"You:\"\n",
        "    conversation_history.append(f\"You: {user_input}\")\n",
        "\n",
        "    # Generate the chatbot response using updated history\n",
        "    response = generate_response_with_history(conversation_history,\n",
        "                        params['max_resp_token'], params['temperature'],\n",
        "                        params['num_beams'], params['top_k'],\n",
        "                        params['top_p'], params['rep_penalty'])\n",
        "\n",
        "    # Add model response to history with prefix \"Me:\"\n",
        "    conversation_history.append(f\"Me: {response}\")\n",
        "\n",
        "    # Limit conversation history to \"max_hist_conv\" exchanges\n",
        "    if len(conversation_history) > params['max_hist_conv']:\n",
        "        conversation_history = conversation_history[-params['max_hist_conv']:]\n",
        "\n",
        "    # Truncate conversation history if token length exceeds 'max_hist_token'\n",
        "    conversation_history = truncate_conversation_history(conversation_history, \\\n",
        "                                tokenizer, params['max_hist_token'])\n",
        "    # Return chatbot's response to be displayed in the interface\n",
        "    return response\n",
        "\n",
        "# Gradio Interface for user interaction with the chatbot\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,         # Function call when the user submits input\n",
        "    inputs=\"text\",      # Plain text input\n",
        "    outputs=\"text\",     # Plain text output retuned\n",
        "    title=\"USD-AAI-520 Group 3 FLAN-T5 Chatbot\",\n",
        "    description=\"I'm a FLAN-T5 model! I will talk to you till you say 'bye' :).\"\n",
        ")\n",
        "\n",
        "# Launch Gradio Interface allowing users to interact with the chatbot\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5810882,
          "sourceId": 9539464,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30776,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
